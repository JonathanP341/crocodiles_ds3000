{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# XGBoost + CatBoost Classification for Crocodile Species\n",
        "# Baseline, Bayesian tuning with Optuna, Feature Engineering, Class Imbalance Handling, and Feature Importance\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import log_loss, accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "import xgboost as xgb\n",
        "import optuna\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "# Load dataset from root CSV\n",
        "DATA_PATH = \"crocodile_dataset.csv\"\n",
        "raw_df = pd.read_csv(DATA_PATH)\n",
        "raw_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Shape:\", raw_df.shape)\n",
        "display(raw_df.head())\n",
        "print(\"\\nInfo:\")\n",
        "print(raw_df.info())\n",
        "\n",
        "print(\"\\nClass distribution (Common Name):\")\n",
        "print(raw_df['Common Name'].value_counts(normalize=True))\n",
        "\n",
        "# Example feature distribution plots for a couple of numeric columns\n",
        "numeric_cols = raw_df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "if numeric_cols:\n",
        "    raw_df[numeric_cols].hist(figsize=(12, 8))\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Correlation heatmap for numerical features\n",
        "if len(numeric_cols) <= 20 and len(numeric_cols) > 1:\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(raw_df[numeric_cols].corr(), annot=False, cmap='coolwarm')\n",
        "    plt.title('Correlation Heatmap (Numeric Features)')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocessing: remove contested species and leak features, then create train/val split\n",
        "\n",
        "DF = raw_df.copy()\n",
        "\n",
        "# Remove contested species to avoid taxonomic ambiguity\n",
        "# Borneo Crocodile taxonomic status is disputed= excluded\n",
        "CONTESTED_SPECIES_NAME = 'Borneo Crocodile (disputed)'\n",
        "if CONTESTED_SPECIES_NAME in DF['Common Name'].unique():\n",
        "    DF = DF[DF['Common Name'] != CONTESTED_SPECIES_NAME].copy()\n",
        "    print(f\"Removed contested species '{CONTESTED_SPECIES_NAME}', new shape: {DF.shape}\")\n",
        "\n",
        "# Drop columns that directly reveal the target (data leakage)\n",
        "leak_cols = [col for col in ['Observation ID', 'Scientific Name', 'Family', 'Genus', \n",
        "                              'Conservation Status', 'Observer Name', 'Notes'] if col in DF.columns]\n",
        "print(\"Dropping leak columns:\", leak_cols)\n",
        "DF = DF.drop(columns=leak_cols)\n",
        "\n",
        "\n",
        "if 'Date of Observation' in DF.columns:\n",
        "    DF['Date of Observation'] = pd.to_datetime(DF['Date of Observation'], dayfirst=True, errors='coerce')\n",
        "    DF['Year'] = DF['Date of Observation'].dt.year\n",
        "    DF['Month'] = DF['Date of Observation'].dt.month\n",
        "    \n",
        "    # Normalize year using full dataset statistics before train/test split\n",
        "    if DF['Year'].notna().any():\n",
        "        max_year = DF['Year'].max()\n",
        "        min_year = DF['Year'].min()\n",
        "        if max_year != min_year:\n",
        "            DF['Year'] = (DF['Year'] - min_year) / (max_year - min_year)\n",
        "    \n",
        "    # Sine transformation = cyclic nature of months\n",
        "    DF['Month'] = np.sin((np.pi * DF['Month']) / 12)\n",
        "    DF = DF.drop(columns=['Date of Observation'])\n",
        "\n",
        "# Define features and target (predicting Common Name)\n",
        "target_col = 'Common Name'\n",
        "X = DF.drop(columns=[target_col])\n",
        "y = DF[target_col]\n",
        "\n",
        "# Identify column types\n",
        "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "print(\"Categorical columns:\", categorical_cols)\n",
        "print(\"Numeric columns:\", numeric_cols)\n",
        "\n",
        "# METHODOLOGY: Preprocessing pipeline design\n",
        "# Numeric features: median imputation (robust to outliers) + standardization (mean=0, std=1)\n",
        "# Categorical features: mode imputation + one-hot encoding\n",
        "# StandardScaler ensures all numeric features have similar scales for tree-based models\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),  # Median robust to outliers\n",
        "    ('scaler', StandardScaler())  # Standardization for consistent feature scales\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Mode imputation for categoricals\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # One-hot encoding\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Stratified train/validation split\n",
        "# Stratification ensures class distribution is preserved in both sets\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=y  # Preserve class distribution in train/val sets\n",
        ")\n",
        "\n",
        "print(f\"Train shape: {X_train.shape}, Valid shape: {X_valid.shape}\")\n",
        "print(f\"Number of classes: {len(y.unique())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Engineering: Interaction Features and Target Encoding\n",
        "# Create interaction features to capture non-linear relationships\n",
        "\n",
        "\n",
        "X_train_fe = X_train.copy()\n",
        "X_valid_fe = X_valid.copy()\n",
        "\n",
        "# 1. Interaction Features\n",
        "# Create multiplicative and ratio features to capture non-linear relationships\n",
        "# These features may help models learn species-specific size patterns\n",
        "print(\"Creating interaction features...\")\n",
        "\n",
        "# Length × Weight captures overall body size (larger crocodiles)\n",
        "# Multiplicative interaction may reveal species-specific size relationships\n",
        "if 'Observed Length (m)' in X_train_fe.columns and 'Observed Weight (kg)' in X_train_fe.columns:\n",
        "    X_train_fe['Length_x_Weight'] = X_train_fe['Observed Length (m)'] * X_train_fe['Observed Weight (kg)']\n",
        "    X_valid_fe['Length_x_Weight'] = X_valid_fe['Observed Length (m)'] * X_valid_fe['Observed Weight (kg)']\n",
        "\n",
        "# Year × Month captures temporal interaction patterns\n",
        "# May reveal seasonal or temporal trends in observation patterns\n",
        "if 'Year' in X_train_fe.columns and 'Month' in X_train_fe.columns:\n",
        "    X_train_fe['Year_x_Month'] = X_train_fe['Year'] * X_train_fe['Month']\n",
        "    X_valid_fe['Year_x_Month'] = X_valid_fe['Year'] * X_valid_fe['Month']\n",
        "\n",
        "# Weight/Length ratio indicates body condition (mass per unit length)\n",
        "# Different species may have different body proportions\n",
        "if 'Observed Length (m)' in X_train_fe.columns and 'Observed Weight (kg)' in X_train_fe.columns:\n",
        "    X_train_fe['Weight_per_Length'] = X_train_fe['Observed Weight (kg)'] / (X_train_fe['Observed Length (m)'] + 1e-6)\n",
        "    X_valid_fe['Weight_per_Length'] = X_valid_fe['Observed Weight (kg)'] / (X_valid_fe['Observed Length (m)'] + 1e-6)\n",
        "\n",
        "print(f\"Added interaction features. New feature count: {X_train_fe.shape[1]}\")\n",
        "\n",
        "# 2. Target Encoding for High-Cardinality Categorical Features\n",
        "# Using cross-validation to prevent target leakage\n",
        "print(\"\\nApplying target encoding to categorical features...\")\n",
        "\n",
        "try:\n",
        "    from category_encoders import TargetEncoder\n",
        "    use_target_encoder = True\n",
        "except ImportError:\n",
        "    print(\"category_encoders not available. Using manual target encoding with CV...\")\n",
        "    use_target_encoder = False\n",
        "\n",
        "if use_target_encoder:\n",
        "    # Target encode categorical columns using cross-validation\n",
        "    categorical_cols_to_encode = [col for col in ['Country/Region', 'Habitat Type'] \n",
        "                                  if col in X_train_fe.columns]\n",
        "    \n",
        "    if categorical_cols_to_encode:\n",
        "        # Use TargetEncoder with CV to prevent leakage\n",
        "        target_encoder = TargetEncoder(cols=categorical_cols_to_encode, smoothing=1.0)\n",
        "        X_train_fe_encoded = target_encoder.fit_transform(X_train_fe, y_train)\n",
        "        X_valid_fe_encoded = target_encoder.transform(X_valid_fe)\n",
        "        \n",
        "        # Keep original columns and add encoded versions with suffix\n",
        "        for col in categorical_cols_to_encode:\n",
        "            X_train_fe[f'{col}_target_enc'] = X_train_fe_encoded[col]\n",
        "            X_valid_fe[f'{col}_target_enc'] = X_valid_fe_encoded[col]\n",
        "        \n",
        "        print(f\"Target encoded columns: {categorical_cols_to_encode}\")\n",
        "else:\n",
        "    # Manual target encoding with cross-validation\n",
        "    from sklearn.model_selection import KFold\n",
        "    from sklearn.preprocessing import LabelEncoder\n",
        "    \n",
        "    categorical_cols_to_encode = [col for col in ['Country/Region', 'Habitat Type'] \n",
        "                                  if col in X_train_fe.columns]\n",
        "    \n",
        "    if categorical_cols_to_encode:\n",
        "        # Encode target labels to numeric\n",
        "        temp_le = LabelEncoder()\n",
        "        y_train_encoded_for_te = temp_le.fit_transform(y_train)\n",
        "        global_mean = y_train_encoded_for_te.mean()\n",
        "        \n",
        "        # 5-fold CV for target encoding prevents leakage\n",
        "        # Each fold's encoding uses only training data from other folds\n",
        "        kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "        \n",
        "        for col in categorical_cols_to_encode:\n",
        "            # Initialize encoded columns\n",
        "            X_train_fe[f'{col}_target_enc'] = 0.0\n",
        "            X_valid_fe[f'{col}_target_enc'] = 0.0\n",
        "            \n",
        "            for train_idx, val_idx in kf.split(X_train_fe):\n",
        "                train_mean = pd.Series(y_train_encoded_for_te[train_idx]).groupby(\n",
        "                    X_train_fe[col].iloc[train_idx]\n",
        "                ).mean()\n",
        "                X_train_fe.iloc[val_idx, X_train_fe.columns.get_loc(f'{col}_target_enc')] = (\n",
        "                    X_train_fe[col].iloc[val_idx].map(train_mean).fillna(global_mean).values\n",
        "                )\n",
        "            \n",
        "            # Validation set encoded using full training set mean\n",
        "            train_mean = pd.Series(y_train_encoded_for_te).groupby(X_train_fe[col]).mean()\n",
        "            X_valid_fe[f'{col}_target_enc'] = X_valid_fe[col].map(train_mean).fillna(global_mean)\n",
        "        \n",
        "        print(f\"Manually target encoded columns: {categorical_cols_to_encode}\")\n",
        "\n",
        "print(f\"\\nFinal feature count after engineering: {X_train_fe.shape[1]}\")\n",
        "print(\"Feature engineering complete. Using X_train_fe and X_valid_fe for enhanced models.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze class distribution to determine if imbalance handling is needed\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CLASS IMBALANCE ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Analyze class distribution\n",
        "class_counts = y_train.value_counts().sort_index()\n",
        "class_proportions = y_train.value_counts(normalize=True).sort_index()\n",
        "\n",
        "print(\"\\nClass distribution in training set:\")\n",
        "for cls, count in class_counts.items():\n",
        "    prop = class_proportions[cls]\n",
        "    print(f\"  {cls}: {count} samples ({prop:.2%})\")\n",
        "\n",
        "# Compute balanced class weights using inverse frequency\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Create temp encoder for weight calculation\n",
        "temp_label_encoder = LabelEncoder()\n",
        "y_train_for_weights = temp_label_encoder.fit_transform(y_train)\n",
        "# 'balanced' strategy computes n_samples / (n_classes * np.bincount(y))\n",
        "# This gives higher weight to minority classes during training\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_for_weights), y=y_train_for_weights)\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "print(f\"\\nComputed class weights (balanced):\")\n",
        "for i, (cls, weight) in enumerate(zip(temp_label_encoder.classes_, class_weights)):\n",
        "    print(f\"  {cls}: {weight:.3f}\")\n",
        "\n",
        "# Check if imbalance is significant\n",
        "min_prop = class_proportions.min()\n",
        "max_prop = class_proportions.max()\n",
        "imbalance_ratio = max_prop / min_prop\n",
        "\n",
        "print(f\"\\nImbalance ratio (max/min): {imbalance_ratio:.2f}\")\n",
        "if imbalance_ratio > 2.0:\n",
        "    print(\"  -> Significant class imbalance detected. Applying class weights recommended.\")\n",
        "else:\n",
        "    print(\"  -> Classes are relatively balanced. Class weights may still help.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Class imbalance analysis complete.\")\n",
        "print(\"Class weights will be applied to models that support it.\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# XGBoost baseline model\n",
        "\n",
        "# XGBoost needs numeric labels for multi-class classification\n",
        "# LabelEncoder converts string class names -> ints\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "baseline_label_encoder = LabelEncoder()\n",
        "y_train_baseline_encoded = baseline_label_encoder.fit_transform(y_train)\n",
        "y_valid_baseline_encoded = baseline_label_encoder.transform(y_valid)\n",
        "\n",
        "n_classes_baseline = len(baseline_label_encoder.classes_)\n",
        "\n",
        "# Baseline hyperparameters chosen based on common practice\n",
        "xgb_baseline = xgb.XGBClassifier(\n",
        "    objective='multi:softprob',  \n",
        "    eval_metric='mlogloss',  \n",
        "    num_class=n_classes_baseline,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_estimators=100,  \n",
        "    learning_rate=0.1,  \n",
        "    max_depth=5,  \n",
        "    subsample=0.8,  \n",
        "    colsample_bytree=0.8,  \n",
        "    tree_method='hist'\n",
        ")\n",
        "\n",
        "xgb_baseline_pipeline = Pipeline(steps=[\n",
        "    ('preprocess', preprocessor),\n",
        "    ('model', xgb_baseline)\n",
        "])\n",
        "\n",
        "xgb_baseline_pipeline.fit(X_train, y_train_baseline_encoded)\n",
        "\n",
        "# Evaluate on validation set\n",
        "valid_proba = xgb_baseline_pipeline.predict_proba(X_valid)\n",
        "valid_pred_encoded = xgb_baseline_pipeline.predict(X_valid)\n",
        "valid_pred = baseline_label_encoder.inverse_transform(valid_pred_encoded)\n",
        "\n",
        "baseline_logloss = log_loss(y_valid_baseline_encoded, valid_proba)\n",
        "baseline_acc = accuracy_score(y_valid, valid_pred)\n",
        "\n",
        "print(f\"Baseline XGBoost - Log Loss: {baseline_logloss:.4f}, Accuracy: {baseline_acc:.4f}\")\n",
        "print(\"Confusion matrix (baseline):\")\n",
        "print(confusion_matrix(y_valid, valid_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# XGBoost w/ Feature Engineering\n",
        "# Testing if engineered features improve performance\n",
        "\n",
        "# Update preprocessor to handle new features\n",
        "numeric_cols_fe = X_train_fe.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_cols_fe = X_train_fe.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "numeric_transformer_fe = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer_fe = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "\n",
        "preprocessor_fe = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer_fe, numeric_cols_fe),\n",
        "        ('cat', categorical_transformer_fe, categorical_cols_fe)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# XGBoost w/ feature engineering\n",
        "xgb_fe = xgb.XGBClassifier(\n",
        "    objective='multi:softprob',\n",
        "    eval_metric='mlogloss',\n",
        "    num_class=n_classes_baseline,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=5,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    tree_method='hist'\n",
        ")\n",
        "\n",
        "xgb_fe_pipeline = Pipeline(steps=[\n",
        "    ('preprocess', preprocessor_fe),\n",
        "    ('model', xgb_fe)\n",
        "])\n",
        "\n",
        "xgb_fe_pipeline.fit(X_train_fe, y_train_baseline_encoded)\n",
        "\n",
        "# Evaluate\n",
        "valid_proba_fe = xgb_fe_pipeline.predict_proba(X_valid_fe)\n",
        "valid_pred_fe_encoded = xgb_fe_pipeline.predict(X_valid_fe)\n",
        "valid_pred_fe = baseline_label_encoder.inverse_transform(valid_pred_fe_encoded)\n",
        "\n",
        "xgb_fe_logloss = log_loss(y_valid_baseline_encoded, valid_proba_fe)\n",
        "xgb_fe_acc = accuracy_score(y_valid, valid_pred_fe)\n",
        "\n",
        "print(f\"XGBoost with Feature Engineering - Log Loss: {xgb_fe_logloss:.4f}, Accuracy: {xgb_fe_acc:.4f}\")\n",
        "print(\"Confusion matrix (XGBoost + FE):\")\n",
        "print(confusion_matrix(y_valid, valid_pred_fe))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# XGBoost with Class Weights (Handling Imbalance)\n",
        "# Testing if class weights improve performance on minority classes\n",
        "\n",
        "xgb_balanced = xgb.XGBClassifier(\n",
        "    objective='multi:softprob',\n",
        "    eval_metric='mlogloss',\n",
        "    num_class=n_classes_baseline,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=5,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    tree_method='hist',\n",
        "    # Class weights applied via sample_weight parameter in fit() method\n",
        ")\n",
        "\n",
        "xgb_balanced_pipeline = Pipeline(steps=[\n",
        "    ('preprocess', preprocessor),\n",
        "    ('model', xgb_balanced)\n",
        "])\n",
        "\n",
        "# Convert class weights to sample weights for XGBoost\n",
        "sample_weights_train = np.array([class_weight_dict[y] for y in y_train_baseline_encoded])\n",
        "\n",
        "xgb_balanced_pipeline.fit(X_train, y_train_baseline_encoded, model__sample_weight=sample_weights_train)\n",
        "\n",
        "# Evaluate\n",
        "valid_proba_balanced = xgb_balanced_pipeline.predict_proba(X_valid)\n",
        "valid_pred_balanced_encoded = xgb_balanced_pipeline.predict(X_valid)\n",
        "valid_pred_balanced = baseline_label_encoder.inverse_transform(valid_pred_balanced_encoded)\n",
        "\n",
        "xgb_balanced_logloss = log_loss(y_valid_baseline_encoded, valid_proba_balanced)\n",
        "xgb_balanced_acc = accuracy_score(y_valid, valid_pred_balanced)\n",
        "\n",
        "print(f\"XGBoost with Class Weights - Log Loss: {xgb_balanced_logloss:.4f}, Accuracy: {xgb_balanced_acc:.4f}\")\n",
        "print(\"Confusion matrix (XGBoost + Balanced):\")\n",
        "print(confusion_matrix(y_valid, valid_pred_balanced))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optuna Bayesian optimization for XGBoost hyperparameters\n",
        "# Bayesian optimization (Optuna) chosen over GridSearchCV\n",
        "# Optuna uses Tree-structured Parzen Estimator (TPE) to explore hyperparameter space\n",
        "\n",
        "N_TRIALS = 30  # 30 trials balances exploration vs computation time\n",
        "\n",
        "# Encode labels for XGBoost (needed for num_class parameter)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_valid_encoded = label_encoder.transform(y_valid)\n",
        "\n",
        "n_classes = len(label_encoder.classes_)\n",
        "print(f\"Number of classes: {n_classes}\")\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "    # Hyperparameter search space based on XGBoost best practices\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 200),  \n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 11),  \n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),  \n",
        "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),  \n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),  \n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),  \n",
        "        'gamma': trial.suggest_float('gamma', 0.0, 5.0), \n",
        "    }\n",
        "\n",
        "    model = xgb.XGBClassifier(\n",
        "        objective='multi:softprob',\n",
        "        eval_metric='mlogloss',\n",
        "        num_class=n_classes,\n",
        "        random_state=RANDOM_STATE,\n",
        "        tree_method='hist',\n",
        "        **params\n",
        "    )\n",
        "\n",
        "    pipe = Pipeline(steps=[\n",
        "        ('preprocess', preprocessor),\n",
        "        ('model', model)\n",
        "    ])\n",
        "\n",
        "    # 3-fold stratified cross-validation on training set only\n",
        "    # preserves class distribution in each fold\n",
        "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
        "    cv_log_losses = []\n",
        "\n",
        "    for train_idx, valid_idx in skf.split(X_train, y_train_encoded):\n",
        "        X_tr, X_va = X_train.iloc[train_idx], X_train.iloc[valid_idx]\n",
        "        y_tr, y_va = y_train_encoded[train_idx], y_train_encoded[valid_idx]\n",
        "\n",
        "        pipe.fit(X_tr, y_tr)\n",
        "        proba_va = pipe.predict_proba(X_va)\n",
        "        # Log loss used as optimization metric\n",
        "        cv_log_losses.append(log_loss(y_va, proba_va))\n",
        "\n",
        "    return np.mean(cv_log_losses)\n",
        "\n",
        "\n",
        "# TPE (Tree-structured Parzen Estimator) sampler for Bayesian optimization\n",
        "study = optuna.create_study(\n",
        "    direction='minimize',  # minimize log loss\n",
        "    study_name='xgb_crocodiles_opt',\n",
        "    sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE)\n",
        ")\n",
        "study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=False)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "print(f\"Best CV log loss: {study.best_trial.value:.4f}\")\n",
        "print(\"Best parameters:\")\n",
        "print(study.best_trial.params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train final XGBoost model with best Optuna parameters and evaluate\n",
        "\n",
        "best_params = study.best_trial.params\n",
        "best_xgb = xgb.XGBClassifier(\n",
        "    objective='multi:softprob',\n",
        "    eval_metric='mlogloss',\n",
        "    num_class=n_classes,\n",
        "    random_state=RANDOM_STATE,\n",
        "    tree_method='hist',\n",
        "    **best_params\n",
        ")\n",
        "\n",
        "xgb_best_pipeline = Pipeline(steps=[\n",
        "    ('preprocess', preprocessor),\n",
        "    ('model', best_xgb)\n",
        "])\n",
        "\n",
        "# Fit on training set, evaluate on validation set (same split as baseline)\n",
        "xgb_best_pipeline.fit(X_train, y_train_encoded)\n",
        "\n",
        "valid_proba_best = xgb_best_pipeline.predict_proba(X_valid)\n",
        "valid_pred_best_encoded = xgb_best_pipeline.predict(X_valid)\n",
        "valid_pred_best = label_encoder.inverse_transform(valid_pred_best_encoded)\n",
        "\n",
        "best_logloss = log_loss(y_valid_encoded, valid_proba_best)\n",
        "best_acc = accuracy_score(y_valid, valid_pred_best)\n",
        "\n",
        "print(f\"Tuned XGBoost - Log Loss: {best_logloss:.4f}, Accuracy: {best_acc:.4f}\")\n",
        "print(\"Confusion matrix (tuned):\")\n",
        "print(confusion_matrix(y_valid, valid_pred_best))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CatBoost Implementation\n",
        "# CatBoost chosen as alternative gradient boosting algorithm\n",
        "# Key advantage: native categorical feature handling without one-hot encoding\n",
        "# This may reduce dimensionality and improve performance on categorical features\n",
        "\n",
        "try:\n",
        "    from catboost import CatBoostClassifier\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    print(\"CATBOOST IMPLEMENTATION\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Identify categorical columns for CatBoost\n",
        "    cat_features_indices = [i for i, col in enumerate(X_train.columns) \n",
        "                            if col in categorical_cols]\n",
        "    \n",
        "    print(f\"\\nCategorical features (indices): {cat_features_indices}\")\n",
        "    print(f\"Categorical feature names: {[X_train.columns[i] for i in cat_features_indices]}\")\n",
        "    \n",
        "    # CatBoost Baseline\n",
        "    catboost_baseline = CatBoostClassifier(\n",
        "        iterations=100,\n",
        "        learning_rate=0.1,\n",
        "        depth=5,\n",
        "        loss_function='MultiClass',\n",
        "        random_seed=RANDOM_STATE,\n",
        "        verbose=False,\n",
        "        cat_features=cat_features_indices if cat_features_indices else None\n",
        "    )\n",
        "    \n",
        "    # CatBoost can handle raw DataFrames with categoricals\n",
        "    catboost_baseline.fit(\n",
        "        X_train, y_train_baseline_encoded,\n",
        "        cat_features=cat_features_indices if cat_features_indices else None,\n",
        "        verbose=False\n",
        "    )\n",
        "    \n",
        "    # Evaluate\n",
        "    catboost_baseline_proba = catboost_baseline.predict_proba(X_valid)\n",
        "    catboost_baseline_pred_encoded = catboost_baseline.predict(X_valid)\n",
        "    catboost_baseline_pred = baseline_label_encoder.inverse_transform(catboost_baseline_pred_encoded)\n",
        "    \n",
        "    catboost_baseline_logloss = log_loss(y_valid_baseline_encoded, catboost_baseline_proba)\n",
        "    catboost_baseline_acc = accuracy_score(y_valid, catboost_baseline_pred)\n",
        "    \n",
        "    print(f\"\\nCatBoost Baseline - Log Loss: {catboost_baseline_logloss:.4f}, Accuracy: {catboost_baseline_acc:.4f}\")\n",
        "    print(\"Confusion matrix (CatBoost Baseline):\")\n",
        "    print(confusion_matrix(y_valid, catboost_baseline_pred))\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"CatBoost not installed. Install with: pip install catboost\")\n",
        "    catboost_baseline = None\n",
        "    catboost_baseline_logloss = None\n",
        "    catboost_baseline_acc = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CatBoost w/ Optuna Hyperparameter Tuning\n",
        "\n",
        "SKIP_CATBOOST_TUNING = False  # Set to True to skip tuning (use baseline only), cause it can be slow\n",
        "\n",
        "try:\n",
        "    from catboost import CatBoostClassifier\n",
        "    \n",
        "    if catboost_baseline is not None and not SKIP_CATBOOST_TUNING:\n",
        "        print(\"=\"*70)\n",
        "        print(\"CATBOOST OPTUNA TUNING\")\n",
        "        print(\"=\"*70)\n",
        "        print(\"NOTE: This may take several minutes. Interrupt if needed.\")\n",
        "        print(\"You can skip this by setting SKIP_CATBOOST_TUNING = True above.\")\n",
        "        print(\"=\"*70)\n",
        "        \n",
        "        # Same num of trials as XGBoost for fair comparison\n",
        "        N_TRIALS_CAT = 30\n",
        "        # Limited iterations during tuning to reduce computation time\n",
        "        MAX_ITERATIONS = 50\n",
        "        \n",
        "        def catboost_objective(trial):\n",
        "            params = {\n",
        "                'iterations': trial.suggest_int('iterations', 30, MAX_ITERATIONS),\n",
        "                'depth': trial.suggest_int('depth', 3, 7),\n",
        "                'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.2),\n",
        "                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 5),\n",
        "                'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli']),\n",
        "                'random_strength': trial.suggest_float('random_strength', 0.5, 1.5),\n",
        "            }\n",
        "            \n",
        "            if params['bootstrap_type'] == 'Bayesian':\n",
        "                params['bagging_temperature'] = trial.suggest_float('bagging_temperature', 0.5, 1.0)\n",
        "            elif params['bootstrap_type'] == 'Bernoulli':\n",
        "                params['subsample'] = trial.suggest_float('subsample', 0.7, 1.0)\n",
        "            \n",
        "            # 2-fold CV used instead of 3-fold to reduce computation time\n",
        "            skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=RANDOM_STATE)\n",
        "            cv_log_losses = []\n",
        "            \n",
        "            # Ensure y is 1D array\n",
        "            y_train_flat = np.array(y_train_baseline_encoded).ravel()\n",
        "            \n",
        "            for train_idx, valid_idx in skf.split(X_train, y_train_flat):\n",
        "                X_tr, X_va = X_train.iloc[train_idx], X_train.iloc[valid_idx]\n",
        "                y_tr = y_train_flat[train_idx]\n",
        "                y_va = y_train_flat[valid_idx]\n",
        "                \n",
        "                # Create a fresh model for each fold\n",
        "                fold_model = CatBoostClassifier(\n",
        "                    loss_function='MultiClass',\n",
        "                    random_seed=RANDOM_STATE,\n",
        "                    verbose=False,\n",
        "                    cat_features=cat_features_indices if cat_features_indices else None,\n",
        "                    early_stopping_rounds=5,\n",
        "                    iterations=params['iterations'], \n",
        "                    **{k: v for k, v in params.items() if k != 'iterations'}\n",
        "                )\n",
        "                \n",
        "                fold_model.fit(X_tr, y_tr, verbose=False)\n",
        "                proba_va = fold_model.predict_proba(X_va)\n",
        "                cv_log_losses.append(log_loss(y_va, proba_va))\n",
        "            \n",
        "            return np.mean(cv_log_losses)\n",
        "        \n",
        "        try:\n",
        "            # Same sampler and seed as XGBoost for reproducibility and fair comparison\n",
        "            catboost_study = optuna.create_study(\n",
        "                direction='minimize', \n",
        "                study_name='catboost_crocodiles_opt',\n",
        "                sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE)\n",
        "            )\n",
        "            catboost_study.optimize(catboost_objective, n_trials=N_TRIALS_CAT, show_progress_bar=False)\n",
        "            \n",
        "            print(\"Best trial:\")\n",
        "            print(f\"Best CV log loss: {catboost_study.best_trial.value:.4f}\")\n",
        "            print(\"Best parameters:\")\n",
        "            print(catboost_study.best_trial.params)\n",
        "            \n",
        "            # Train final CatBoost model with best parameters\n",
        "            best_catboost_params = catboost_study.best_trial.params.copy()\n",
        "            # Handle bootstrap-specific params\n",
        "            bootstrap_type = best_catboost_params.pop('bootstrap_type', 'Bayesian')\n",
        "            if 'bagging_temperature' in best_catboost_params:\n",
        "                best_catboost_params['bootstrap_type'] = 'Bayesian'\n",
        "            elif 'subsample' in best_catboost_params:\n",
        "                best_catboost_params['bootstrap_type'] = 'Bernoulli'\n",
        "            else:\n",
        "                best_catboost_params['bootstrap_type'] = bootstrap_type\n",
        "            \n",
        "            # Limit final model iterations too\n",
        "            if 'iterations' in best_catboost_params:\n",
        "                best_catboost_params['iterations'] = min(best_catboost_params['iterations'], 100)\n",
        "            \n",
        "            catboost_tuned = CatBoostClassifier(\n",
        "                loss_function='MultiClass',\n",
        "                random_seed=RANDOM_STATE,\n",
        "                verbose=False,\n",
        "                cat_features=cat_features_indices if cat_features_indices else None,\n",
        "                early_stopping_rounds=10,\n",
        "                **best_catboost_params\n",
        "            )\n",
        "            \n",
        "            # Ensure y is 1D array\n",
        "            y_train_catboost = np.array(y_train_baseline_encoded).ravel()\n",
        "            catboost_tuned.fit(X_train, y_train_catboost, verbose=False)\n",
        "            \n",
        "            # Evaluate\n",
        "            catboost_tuned_proba = catboost_tuned.predict_proba(X_valid)\n",
        "            catboost_tuned_pred_encoded = catboost_tuned.predict(X_valid)\n",
        "            catboost_tuned_pred = baseline_label_encoder.inverse_transform(catboost_tuned_pred_encoded)\n",
        "            \n",
        "            catboost_tuned_logloss = log_loss(y_valid_baseline_encoded, catboost_tuned_proba)\n",
        "            catboost_tuned_acc = accuracy_score(y_valid, catboost_tuned_pred)\n",
        "            \n",
        "            print(f\"\\nCatBoost Tuned - Log Loss: {catboost_tuned_logloss:.4f}, Accuracy: {catboost_tuned_acc:.4f}\")\n",
        "            print(\"Confusion matrix (CatBoost Tuned):\")\n",
        "            print(confusion_matrix(y_valid, catboost_tuned_pred))\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nCatBoost tuning interrupted. Using baseline only.\")\n",
        "            catboost_tuned_logloss = None\n",
        "            catboost_tuned_acc = None\n",
        "    elif SKIP_CATBOOST_TUNING:\n",
        "        print(\"CatBoost tuning skipped (SKIP_CATBOOST_TUNING = True). Using baseline only.\")\n",
        "        catboost_tuned_logloss = None\n",
        "        catboost_tuned_acc = None\n",
        "    else:\n",
        "        print(\"CatBoost baseline not available. Skipping tuning.\")\n",
        "        catboost_tuned_logloss = None\n",
        "        catboost_tuned_acc = None\n",
        "        \n",
        "except (ImportError, NameError):\n",
        "    print(\"CatBoost not available. Skipping tuning.\")\n",
        "    catboost_tuned_logloss = None\n",
        "    catboost_tuned_acc = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Importance Analysis\n",
        "# Compare feature importances across different models to understand what drives predictions\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Get feature names from preprocessor\n",
        "preprocessor_fitted = xgb_best_pipeline.named_steps['preprocess']\n",
        "feature_names = []\n",
        "\n",
        "# Extract feature names from ColumnTransformer\n",
        "if hasattr(preprocessor_fitted, 'transformers_'):\n",
        "    for name, transformer, cols in preprocessor_fitted.transformers_:\n",
        "        if name == 'num':\n",
        "            feature_names.extend(cols)\n",
        "        elif name == 'cat':\n",
        "            # For one-hot encoded features, get the feature names\n",
        "            if hasattr(transformer.named_steps['onehot'], 'get_feature_names_out'):\n",
        "                cat_feature_names = transformer.named_steps['onehot'].get_feature_names_out(cols)\n",
        "                feature_names.extend(cat_feature_names)\n",
        "            else:\n",
        "                # Fallback: use column names\n",
        "                feature_names.extend([f\"{col}_{i}\" for i, col in enumerate(cols)])\n",
        "\n",
        "# XGBoost Feature Importance\n",
        "xgb_tuned_model = xgb_best_pipeline.named_steps['model']\n",
        "xgb_importance = xgb_tuned_model.feature_importances_\n",
        "\n",
        "# Create DataFrame for easier visualization\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names[:len(xgb_importance)],\n",
        "    'XGBoost_Tuned_Importance': xgb_importance\n",
        "}).sort_values('XGBoost_Tuned_Importance', ascending=False)\n",
        "\n",
        "# Also get baseline XGBoost importance for comparison\n",
        "xgb_baseline_model = xgb_baseline_pipeline.named_steps['model']\n",
        "xgb_baseline_importance = xgb_baseline_model.feature_importances_\n",
        "importance_df['XGBoost_Baseline_Importance'] = xgb_baseline_importance[:len(importance_df)]\n",
        "\n",
        "# Get top 15 features\n",
        "top_features = importance_df.head(15)\n",
        "\n",
        "print(\"\\nTop 15 Most Important Features (Tuned XGBoost):\")\n",
        "print(top_features[['Feature', 'XGBoost_Tuned_Importance']].to_string(index=False))\n",
        "\n",
        "# Visualize feature importance\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "# Plot 1: Top features for tuned XGBoost\n",
        "axes[0].barh(range(len(top_features)), top_features['XGBoost_Tuned_Importance'].values[::-1])\n",
        "axes[0].set_yticks(range(len(top_features)))\n",
        "axes[0].set_yticklabels(top_features['Feature'].values[::-1], fontsize=9)\n",
        "axes[0].set_xlabel('Feature Importance', fontsize=11)\n",
        "axes[0].set_title('Top 15 Features - XGBoost Tuned', fontsize=12, fontweight='bold')\n",
        "axes[0].grid(axis='x', alpha=0.3)\n",
        "\n",
        "# Plot 2: Comparison between baseline and tuned\n",
        "top_10 = importance_df.head(10)\n",
        "x_pos = np.arange(len(top_10))\n",
        "width = 0.35\n",
        "\n",
        "axes[1].barh(x_pos - width/2, top_10['XGBoost_Baseline_Importance'].values, \n",
        "              width, label='Baseline', alpha=0.8)\n",
        "axes[1].barh(x_pos + width/2, top_10['XGBoost_Tuned_Importance'].values, \n",
        "              width, label='Tuned', alpha=0.8)\n",
        "axes[1].set_yticks(x_pos)\n",
        "axes[1].set_yticklabels(top_10['Feature'].values, fontsize=9)\n",
        "axes[1].set_xlabel('Feature Importance', fontsize=11)\n",
        "axes[1].set_title('Feature Importance: Baseline vs Tuned', fontsize=12, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# CatBoost feature importance if available\n",
        "if 'catboost_tuned' in locals() and catboost_tuned is not None:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"CATBOOST FEATURE IMPORTANCE\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    catboost_importance = catboost_tuned.get_feature_importance()\n",
        "    # CatBoost uses original feature indices, need to map to feature names\n",
        "    catboost_importance_df = pd.DataFrame({\n",
        "        'Feature': [X_train.columns[i] if i < len(X_train.columns) else f'Feature_{i}' \n",
        "                   for i in range(len(catboost_importance))],\n",
        "        'CatBoost_Importance': catboost_importance\n",
        "    }).sort_values('CatBoost_Importance', ascending=False)\n",
        "    \n",
        "    print(\"\\nTop 15 Most Important Features (CatBoost Tuned):\")\n",
        "    print(catboost_importance_df.head(15)[['Feature', 'CatBoost_Importance']].to_string(index=False))\n",
        "    \n",
        "    # Plot CatBoost importance\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    top_catboost = catboost_importance_df.head(15)\n",
        "    plt.barh(range(len(top_catboost)), top_catboost['CatBoost_Importance'].values[::-1])\n",
        "    plt.yticks(range(len(top_catboost)), top_catboost['Feature'].values[::-1], fontsize=9)\n",
        "    plt.xlabel('Feature Importance', fontsize=11)\n",
        "    plt.title('Top 15 Features - CatBoost Tuned', fontsize=12, fontweight='bold')\n",
        "    plt.grid(axis='x', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Feature importance analysis complete.\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze how confident the models are in their predictions\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"PREDICTION CONFIDENCE ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Get prediction probabilities for all models\n",
        "models_to_analyze = []\n",
        "\n",
        "# XGBoost Baseline\n",
        "if 'xgb_baseline_pipeline' in locals():\n",
        "    xgb_baseline_proba = xgb_baseline_pipeline.predict_proba(X_valid)\n",
        "    models_to_analyze.append(('XGBoost Baseline', xgb_baseline_proba))\n",
        "\n",
        "# XGBoost Tuned\n",
        "if 'xgb_best_pipeline' in locals():\n",
        "    xgb_tuned_proba = xgb_best_pipeline.predict_proba(X_valid)\n",
        "    models_to_analyze.append(('XGBoost Tuned', xgb_tuned_proba))\n",
        "\n",
        "# CatBoost Baseline\n",
        "if 'catboost_baseline' in locals() and catboost_baseline is not None:\n",
        "    catboost_baseline_proba = catboost_baseline.predict_proba(X_valid)\n",
        "    models_to_analyze.append(('CatBoost Baseline', catboost_baseline_proba))\n",
        "\n",
        "# CatBoost Tuned\n",
        "if 'catboost_tuned' in locals() and catboost_tuned is not None:\n",
        "    catboost_tuned_proba = catboost_tuned.predict_proba(X_valid)\n",
        "    models_to_analyze.append(('CatBoost Tuned', catboost_tuned_proba))\n",
        "\n",
        "# Calculate max probability (confidence) for each prediction\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, (model_name, proba) in enumerate(models_to_analyze[:4]):\n",
        "    max_proba = np.max(proba, axis=1)\n",
        "    \n",
        "    axes[idx].hist(max_proba, bins=20, edgecolor='black', alpha=0.7)\n",
        "    axes[idx].set_xlabel('Maximum Prediction Probability (Confidence)', fontsize=10)\n",
        "    axes[idx].set_ylabel('Number of Samples', fontsize=10)\n",
        "    axes[idx].set_title(f'{model_name}\\nMean Confidence: {max_proba.mean():.3f}', fontsize=11, fontweight='bold')\n",
        "    axes[idx].grid(axis='y', alpha=0.3)\n",
        "    axes[idx].axvline(max_proba.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {max_proba.mean():.3f}')\n",
        "    axes[idx].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\nPrediction Confidence Summary:\")\n",
        "print(\"-\" * 70)\n",
        "for model_name, proba in models_to_analyze:\n",
        "    max_proba = np.max(proba, axis=1)\n",
        "    print(f\"{model_name:20s} | Mean: {max_proba.mean():.4f} | Std: {max_proba.std():.4f} | Min: {max_proba.min():.4f} | Max: {max_proba.max():.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive Model Comparison and Interpretation\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"COMPREHENSIVE MODEL PERFORMANCE SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Build results list\n",
        "results_list = [\n",
        "    {\n",
        "        'Model': 'XGBoost Baseline',\n",
        "        'Log Loss': baseline_logloss,\n",
        "        'Accuracy': baseline_acc,\n",
        "        'Notes': 'Original baseline model'\n",
        "    },\n",
        "    {\n",
        "        'Model': 'XGBoost Tuned (Optuna)',\n",
        "        'Log Loss': best_logloss,\n",
        "        'Accuracy': best_acc,\n",
        "        'Notes': 'Bayesian optimized hyperparameters'\n",
        "    }\n",
        "]\n",
        "\n",
        "if 'xgb_fe_logloss' in locals() and xgb_fe_logloss is not None:\n",
        "    results_list.append({\n",
        "        'Model': 'XGBoost + Feature Engineering',\n",
        "        'Log Loss': xgb_fe_logloss,\n",
        "        'Accuracy': xgb_fe_acc,\n",
        "        'Notes': 'With interaction features & target encoding'\n",
        "    })\n",
        "\n",
        "if 'xgb_balanced_logloss' in locals() and xgb_balanced_logloss is not None:\n",
        "    results_list.append({\n",
        "        'Model': 'XGBoost + Class Weights',\n",
        "        'Log Loss': xgb_balanced_logloss,\n",
        "        'Accuracy': xgb_balanced_acc,\n",
        "        'Notes': 'With balanced class weights'\n",
        "    })\n",
        "\n",
        "if 'catboost_baseline_logloss' in locals() and catboost_baseline_logloss is not None:\n",
        "    results_list.append({\n",
        "        'Model': 'CatBoost Baseline',\n",
        "        'Log Loss': catboost_baseline_logloss,\n",
        "        'Accuracy': catboost_baseline_acc,\n",
        "        'Notes': 'Native categorical handling'\n",
        "    })\n",
        "\n",
        "if 'catboost_tuned_logloss' in locals() and catboost_tuned_logloss is not None:\n",
        "    results_list.append({\n",
        "        'Model': 'CatBoost Tuned (Optuna)',\n",
        "        'Log Loss': catboost_tuned_logloss,\n",
        "        'Accuracy': catboost_tuned_acc,\n",
        "        'Notes': 'Bayesian optimized'\n",
        "    })\n",
        "\n",
        "\n",
        "results_summary = pd.DataFrame(results_list)\n",
        "\n",
        "print(\"\\nResults Table (sorted by Accuracy):\")\n",
        "results_sorted = results_summary.sort_values('Accuracy', ascending=False)\n",
        "print(results_sorted.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"KEY OBSERVATIONS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Find best models\n",
        "best_acc_model = results_summary.loc[results_summary['Accuracy'].idxmax()]\n",
        "best_logloss_model = results_summary.loc[results_summary['Log Loss'].idxmin()]\n",
        "\n",
        "print(f\"\\n1. Best Accuracy: {best_acc_model['Model']} ({best_acc_model['Accuracy']:.2%})\")\n",
        "print(f\"2. Best Log Loss: {best_logloss_model['Model']} ({best_logloss_model['Log Loss']:.4f})\")\n",
        "\n",
        "print(f\"\\n3. XGBoost Baseline: {baseline_acc:.2%} accuracy, {baseline_logloss:.4f} log loss\")\n",
        "print(f\"4. XGBoost Tuned: {best_acc:.2%} accuracy, {best_logloss:.4f} log loss\")\n",
        "if 'xgb_fe_logloss' in locals() and xgb_fe_logloss is not None:\n",
        "    print(f\"5. XGBoost + FE: {xgb_fe_acc:.2%} accuracy, {xgb_fe_logloss:.4f} log loss\")\n",
        "    fe_improvement = xgb_fe_acc - baseline_acc\n",
        "    if fe_improvement > 0:\n",
        "        print(f\"   -> Feature engineering improved accuracy by {fe_improvement:.2%}\")\n",
        "    else:\n",
        "        print(f\"   -> Feature engineering decreased accuracy by {abs(fe_improvement):.2%}\")\n",
        "\n",
        "if 'catboost_baseline_logloss' in locals() and catboost_baseline_logloss is not None:\n",
        "    print(f\"6. CatBoost Baseline: {catboost_baseline_acc:.2%} accuracy, {catboost_baseline_logloss:.4f} log loss\")\n",
        "    catboost_vs_xgb = catboost_baseline_acc - baseline_acc\n",
        "    if catboost_vs_xgb > 0:\n",
        "        print(f\"   -> CatBoost outperforms XGBoost baseline by {catboost_vs_xgb:.2%}\")\n",
        "    else:\n",
        "        print(f\"   -> XGBoost baseline outperforms CatBoost by {abs(catboost_vs_xgb):.2%}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RECOMMENDATIONS\")\n",
        "print(\"=\"*70)\n",
        "print(\"Based on the results, use the model with the best balance of accuracy and log loss.\")\n",
        "print(\"If accuracy drops with new features, revert to the baseline XGBoost model.\")\n",
        "print(\"=\"*70)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
